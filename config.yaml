model:
  embedding_dim: 128
  hidden_dim: 128
  num_decoder_layers: 2
  dropout: 0.1
  nhead: 8
  max_seq_len: 512
  pad_token_id: 0
  sos_token_id: 1
  eos_token_id: 2

data:
  data_dir: data
  batch_size: 16
  num_workers: 4
  train_val_test_split: [0.8, 0.1, 0.1]
  img_size: 224

training:
  seed: 42
  learning_rate: 1e-3
  max_epochs: 50
  gpus: [0]  # Set to [] for CPU
  weight_decay: 1e-4
  patience: 10
  precision: 16  # Options: 16, 32
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1

logging:
  log_dir: logs
  experiment_name: latex_ocr
  use_wandb: false
  wandb_project: latex_ocr
  log_every_n_steps: 50

hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: true